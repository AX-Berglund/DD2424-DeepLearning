{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add src directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current path is /Users/axhome/AX/MASTER/Courses/DD2424/DD2424-DeepLearning/Assignment1/notebooks\n",
    "# we want to add the path /Users/axhome/AX/MASTER/Courses/DD2424/DD2424-DeepLearning/Assignment1/src\n",
    "\n",
    "# add the path to the sys.path\n",
    "# sys.path.append('/Users/axhome/AX/MASTER/Courses/DD2424/DD2424-DeepLearning/Assignment1/src')\n",
    "# or use os.path.abspath\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import functions from src files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import LoadBatch\n",
    "from preprocessing import reshape_images, normalize_images\n",
    "from torch_gradient_computations import ComputeGradsWithTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Training a multi-linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 \n",
    "**Write a function that reads in the data from a CIFAR-10 batch file and returns the image and label data in separate files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 10000)\n"
     ]
    }
   ],
   "source": [
    "X, Y, y = LoadBatch(1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(Y[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 9 9 ... 1 1 5]\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in Train, Validation and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, Y_train, y_train = LoadBatch(1)\n",
    "X_val_raw, Y_val, y_val = LoadBatch(2)\n",
    "X_test_raw, Y_test, y_test = LoadBatch(\"test_batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "**pre-process the raw input data**\n",
    "\n",
    "Compute the mean and standard deviation vector for the\n",
    "training data and then normalize the training, validation and test data\n",
    "w.r.t. the training mean and standard deviation vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform training data to have zero mean\n",
    "\n",
    "X_train_mean = np.mean(X_train_raw, axis=1).reshape(-1, 1) # (3072, 1)\n",
    "X_train_std = np.std(X_train_raw, axis=1).reshape(-1, 1) # (3072, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "\n",
    "# Training data\n",
    "X_train = X_train_raw - X_train_mean\n",
    "X_train = X_train / X_train_std\n",
    "\n",
    "# Validation data\n",
    "X_val = X_val_raw - X_train_mean\n",
    "X_val = X_val / X_train_std\n",
    "\n",
    "# Testing data\n",
    "X_test = X_test_raw - X_train_mean\n",
    "X_test = X_test / X_train_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "**Initialise the parameters of the model W (Kxd) and b (Kx1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random number generation\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Get the BitGenerator used by deault_rng\n",
    "#PCG64\n",
    "BitGen = type(rng.bit_generator)\n",
    "\n",
    "# use the state from a fresh bit generator \n",
    "seed = 42\n",
    "\n",
    "# dimensions\n",
    "K = 10 \n",
    "d = 3072\n",
    "\n",
    "# set the state of the bit generator\n",
    "rng.bit_generator.state = BitGen(seed).state\n",
    "\n",
    "# initialize the network\n",
    "init_net = {}\n",
    "init_net[\"W\"] = 0.01*rng.standard_normal(size = (K, d))\n",
    "init_net[\"b\"] = np.zeros((K, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "**Write a function that applies the network function, (i.e. equations 1 and 2) to multiple images and returns the results**\n",
    "\n",
    "$$\n",
    "\\mathbf{s} = \\mathbf{W} \\mathbf{x} + \\mathbf{b} \\quad (1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{p} = \\text{SOFTMAX}(\\mathbf{s}) \\quad (2)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 10000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function P = ApplyNetwork(X, net)\n",
    "# def ApplyNetwork(X, net):\n",
    "#     W = net[\"W\"]\n",
    "#     b = net[\"b\"]\n",
    "#     P = W @ X + b\n",
    "#     return P\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "def softmax(S):\n",
    "    \"\"\"\n",
    "    Computes column-wise softmax of S, where S is K x N.\n",
    "    Returns a K x N matrix of probabilities.\n",
    "    \"\"\"\n",
    "    S_shifted = S - np.max(S, axis=0, keepdims=True)  # Numerical stability\n",
    "    exp_S = np.exp(S_shifted)\n",
    "    return exp_S / np.sum(exp_S, axis=0, keepdims=True)\n",
    "\n",
    "def ApplyNetwork(X, net):\n",
    "    \"\"\"\n",
    "    Applies the network function: Computes class scores and applies softmax.\n",
    "    \n",
    "    Parameters:\n",
    "      X   : d x N array of input images (each column is an image)\n",
    "      net : Dictionary containing network parameters\n",
    "            net[\"W\"] (K x d) - Weight matrix\n",
    "            net[\"b\"] (K x 1) - Bias vector\n",
    "    \n",
    "    Returns:\n",
    "      P : K x N matrix of class probabilities after softmax\n",
    "    \"\"\"\n",
    "    W = net[\"W\"]\n",
    "    b = net[\"b\"]\n",
    "    \n",
    "    S = W @ X + b  # Compute raw class scores\n",
    "    P = softmax(S)  # Apply softmax to get probabilities\n",
    "    return P\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = ApplyNetwork(X_train[:, 0:20], init_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 \n",
    "**Write the function that computes the loss function given by equation (5) for a set of images**\n",
    "\n",
    "$$\n",
    "J(\\mathcal{D}, \\lambda, \\mathbf{W}, \\mathbf{b}) =\n",
    "\\frac{1}{|\\mathcal{D}|} \\sum_{(\\mathbf{x}, \\mathbf{y}) \\in \\mathcal{D}}\n",
    "l_{\\text{cross}} (\\mathbf{x}, \\mathbf{y}, \\mathbf{W}, \\mathbf{b})\n",
    "+ \\lambda \\sum_{i,j} W_{ij}^{2} \\quad \\quad \\quad (5)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeLoss(P, y, W, lam):\n",
    "    \"\"\"\n",
    "    Computes the cost function J using softmax probabilities.\n",
    "    \n",
    "    Parameters:\n",
    "      P   : K x N matrix of softmax probabilities\n",
    "      y   : 1D array of length N with ground truth class indices (integers 0 to 9)\n",
    "      W   : K x d weight matrix (needed for regularization term)\n",
    "      lam : Regularization coefficient (float)\n",
    "    \n",
    "    Returns:\n",
    "      J : Scalar value representing the total loss (cross-entropy + regularization)\n",
    "    \"\"\"\n",
    "    # Number of training examples\n",
    "    N = P.shape[1]\n",
    "\n",
    "    # Pick correct class probabilities for each example using indexing\n",
    "    # Example P[y=5,Sample1] is the probability of class 5 for the first example\n",
    "    log_probs = -np.log(P[y, np.arange(N)] + 1e-15)  # Avoid log(0) \n",
    "    \n",
    "    # Cross-entropy loss (mean over all examples)\n",
    "    cross_entropy_loss = np.mean(log_probs)\n",
    "\n",
    "    # Regularization term (L2 penalty on W)\n",
    "    reg_term = lam * np.sum(W**2)\n",
    "\n",
    "    # Total cost\n",
    "    J = cross_entropy_loss + reg_term\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = ComputeLoss(P, y_train[0][0:20], init_net[\"W\"], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6269273667412074"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "**Write a function that computes the accuracy of the networkâ€™s predic-\n",
    "tions given by equation (4) on a set of data**\n",
    "\n",
    "$$\n",
    "k^* = \\arg\\max\\limits_{1 \\leq k \\leq K} \\{p_1, \\dots, p_K\\}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAccuracy(P, y):\n",
    "    \"\"\"\n",
    "    Computes the classification accuracy of the network.\n",
    "    \n",
    "    Parameters:\n",
    "      P : K x N matrix of softmax probabilities (each column sums to 1)\n",
    "      y : 1D array of length N containing ground truth class indices (0 to K-1)\n",
    "    \n",
    "    Returns:\n",
    "      accuracy : Scalar value representing the accuracy percentage\n",
    "    \"\"\"\n",
    "    # Get the predicted class index for each image (argmax over K classes)\n",
    "    y_pred = np.argmax(P, axis=0)  # Shape: (N,)\n",
    "\n",
    "    # Compute the percentage of correct predictions\n",
    "    accuracy = np.mean(y_pred == y) * 100  # Convert to percentage\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc = ComputeAccuracy(P, y_train[0][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7\n",
    "**Write the function that evaluates, for a mini-batch, the gradients of the\n",
    "cost function w.r.t. W and b, that is equations (10, 11).**\n",
    "\n",
    "\n",
    "#### Gradient Equations\n",
    "\n",
    "The partial derivatives of the objective function $ J $ with respect to $ W $ and $ b $ are given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathcal{B}^{(t+1)}, \\lambda, W, b)}{\\partial W} =\n",
    "\\frac{1}{|\\mathcal{B}^{(t+1)}|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{B}^{(t+1)}}\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{cross}}(\\mathbf{x}, y, W, b)}{\\partial W} + 2\\lambda W\n",
    "\\quad (10)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathcal{B}^{(t+1)}, \\lambda, W, b)}{\\partial b} =\n",
    "\\frac{1}{|\\mathcal{B}^{(t+1)}|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{B}^{(t+1)}}\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{cross}}(\\mathbf{x}, y, W, b)}{\\partial b}\n",
    "\\quad (11)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackwardPass(X, Y, P, network, lam):\n",
    "    \"\"\"\n",
    "    Computes gradients of the cost function J with respect to W and b.\n",
    "\n",
    "    Parameters:\n",
    "      X      : d x N input data matrix (each column is an image)\n",
    "      Y      : K x N one-hot ground truth labels\n",
    "      P      : K x N softmax probabilities from ApplyNetwork\n",
    "      network: Dictionary containing model parameters:\n",
    "               - \"W\": K x d weight matrix\n",
    "               - \"b\": K x 1 bias vector\n",
    "      lam    : Regularization coefficient\n",
    "\n",
    "    Returns:\n",
    "      grads: Dictionary with gradients\n",
    "             - \"W\": K x d gradient matrix\n",
    "             - \"b\": K x 1 gradient vector\n",
    "    \"\"\"\n",
    "    N = X.shape[1]  # Number of samples in mini-batch\n",
    "\n",
    "    # Compute gradient of the loss w.r.t. S (logits)\n",
    "    G = P - Y  # (K x N)\n",
    "\n",
    "    # Compute gradients\n",
    "    grad_W = (1 / N) * (G @ X.T) + 2 * lam * network[\"W\"]  # (K x d)\n",
    "    grad_b = (1 / N) * np.sum(G, axis=1, keepdims=True)     # (K x 1)\n",
    "\n",
    "    # Store gradients in a dictionary\n",
    "    grads = {\n",
    "        \"W\": grad_W,\n",
    "        \"b\": grad_b\n",
    "    }\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of W shape: (10, 3072)\n",
      "Gradient of b shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "P = ApplyNetwork(X, init_net)\n",
    "\n",
    "# Compute gradients\n",
    "grads = BackwardPass(X, Y, P, init_net, 0.1)\n",
    "\n",
    "print(\"Gradient of W shape:\", grads[\"W\"].shape)  # Should be (K, d)\n",
    "print(\"Gradient of b shape:\", grads[\"b\"].shape)  # Should be (K, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_small = 10\n",
    "n_small = 3\n",
    "lam = 0\n",
    "small_net = {}\n",
    "small_net['W'] = .01*rng.standard_normal(size = (10, d_small))\n",
    "small_net['b'] = np.zeros((10, 1))\n",
    "X_small = X_train[0:d_small, 0:n_small]\n",
    "Y_small = Y_train[:, 0:n_small]\n",
    "P = ApplyNetwork(X_small, small_net)\n",
    "my_grads = BackwardPass(X_small, Y_small, P, small_net, lam)\n",
    "torch_grads = ComputeGradsWithTorch(X_small, y_train[0][0:n_small], small_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gradients(dict1, dict2, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Compares two dictionaries containing gradients and returns the values\n",
    "    where the absolute difference is greater than the specified tolerance.\n",
    "\n",
    "    Parameters:\n",
    "      dict1 : dict\n",
    "          First dictionary containing gradients.\n",
    "      dict2 : dict\n",
    "          Second dictionary containing gradients.\n",
    "      tolerance : float\n",
    "          The threshold for considering two values as different.\n",
    "\n",
    "    Returns:\n",
    "      differences : dict\n",
    "          A dictionary containing the keys and the corresponding values\n",
    "          where the absolute difference is greater than the tolerance.\n",
    "    \"\"\"\n",
    "    differences = {}\n",
    "    for key in dict1:\n",
    "        diff = np.abs(dict1[key] - dict2[key])\n",
    "        if np.any(diff > tolerance):\n",
    "            differences[key] = diff[diff > tolerance]\n",
    "\n",
    "    if len(differences) == 0:\n",
    "        print(\"Gradients match within the specified tolerance.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Number of gradients that differ: {len(differences)}\")\n",
    "        print(f\"Max difference: {np.max([np.max(diff) for diff in differences.values()])}\")\n",
    "\n",
    "        return differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = compare_gradients(my_grads, torch_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
