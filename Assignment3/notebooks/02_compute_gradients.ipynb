{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Compute Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(conv_outputs_mat, W1, W2, b1, b2, n_p, nf, n):\n",
    "    \"\"\"\n",
    "    Perform forward pass of the network\n",
    "    \"\"\"\n",
    "    # ReLU activation on flattened convolution outputs\n",
    "    conv_flat = np.fmax(conv_outputs_mat.reshape((n_p * nf, n), order='C'), 0)\n",
    "    \n",
    "    # First layer\n",
    "    s1 = W1 @ conv_flat + b1  # shape: (nh × 1)\n",
    "    h = np.maximum(0, s1)     # ReLU activation\n",
    "    \n",
    "    # Second layer (output layer)\n",
    "    s2 = W2 @ h + b2         # shape: (10 × n)\n",
    "    \n",
    "    # Softmax activation\n",
    "    exp_scores = np.exp(s2)\n",
    "    P = exp_scores / np.sum(exp_scores, axis=0)  # shape: (10 × n)\n",
    "    \n",
    "    return {\n",
    "        'conv_flat': conv_flat,\n",
    "        'h': h,\n",
    "        'P': P\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(Y, P, h, conv_flat, W1, W2, conv_outputs_mat, n_p, nf, n, MX):\n",
    "    \"\"\"\n",
    "    Perform backward pass to compute gradients according to the backpropagation equations\n",
    "    from the assignment description.\n",
    "    \"\"\"\n",
    "    # Compute gradients for the second layer (output layer)\n",
    "    # This corresponds to the gradient of cross-entropy loss w.r.t. the softmax output\n",
    "    # The gradient is -(Y - P) as per standard cross-entropy derivative\n",
    "    # See equation (24) for label smoothing version: -(y_smooth - p)\n",
    "    G_batch = -(Y - P)  # shape: (10 × n)\n",
    "    \n",
    "    # Apply batch normalization as per equation (20) and (21) where we divide by batch size |B|\n",
    "    # L(B, Θ) = (1/|B|) * sum of losses over the batch\n",
    "    grad_W2 = (G_batch @ h.T) / n  # Division by batch size n implements the (1/|B|) term\n",
    "    grad_b2 = np.sum(G_batch, axis=1, keepdims=True) / n  # Sum over batch, then normalize\n",
    "\n",
    "    # Compute gradients for the first layer (hidden layer)\n",
    "    # Backpropagate the gradient through W2\n",
    "    G_batch = W2.T @ G_batch\n",
    "    # Apply ReLU gradient (derivative is 1 for positive values, 0 otherwise)\n",
    "    # This corresponds to the derivative of equation (3): max(0, W_1 h + b_1)\n",
    "    G_batch = G_batch * (h > 0)  # ReLU gradient\n",
    "    grad_W1 = (G_batch @ conv_flat.T) / n  # Normalize by batch size\n",
    "    grad_b1 = np.sum(G_batch, axis=1, keepdims=True) / n  # Normalize by batch size\n",
    "\n",
    "    # Compute gradients for the filters (convolution layer)\n",
    "    # Backpropagate the gradient through W1\n",
    "    G_batch = W1.T @ G_batch\n",
    "    # Apply ReLU gradient for the convolution outputs\n",
    "    # This corresponds to the derivative of equation (1): max(0, X * F_i)\n",
    "    G_batch = G_batch * (conv_flat > 0)  # ReLU gradient\n",
    "    # Reshape to match the expected dimensions for einsum\n",
    "    # This matches the shape of G in equation (19): G = [g_1, g_2, g_3]\n",
    "    GG = G_batch.reshape((n_p, nf, n), order='C')\n",
    "\n",
    "    # Compute gradients for the filters using Einstein summation\n",
    "    # Transpose MX to prepare for the operation in equation (18): ∂L/∂F_all = MX^T * G\n",
    "    MXt = np.transpose(MX, (1, 0, 2))\n",
    "    \n",
    "    # This implements equation (22): F^grad_all = (1/n) * sum_i=1^n [M(:,:,i)^T * G(:,:,i)]\n",
    "    # The einsum operation performs the matrix multiplication for each sample in the batch\n",
    "    # and the division by n implements the (1/|B|) term from equation (21)\n",
    "    grad_Fs_flat = np.einsum('ijn,jln->il', MXt, GG, optimize=True) / n\n",
    "\n",
    "    return {\n",
    "        'grad_W2': grad_W2,\n",
    "        'grad_b2': grad_b2,\n",
    "        'grad_W1': grad_W1,\n",
    "        'grad_b1': grad_b1,\n",
    "        'grad_Fs_flat': grad_Fs_flat\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y, P):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss\n",
    "    \"\"\"\n",
    "    return -np.sum(Y * np.log(P)) / Y.shape[1]\n",
    "\n",
    "def verify_gradients(computed_grads, load_data):\n",
    "    \"\"\"\n",
    "    Verify computed gradients against provided data\n",
    "    \"\"\"\n",
    "    print(\"Verifying grad_Fs_flat...\")\n",
    "    diff = np.abs(computed_grads['grad_Fs_flat'] - load_data['grad_Fs_flat'])\n",
    "    print(f\"Maximum absolute difference: {np.max(diff)}\")\n",
    "    print(f\"Average absolute difference: {np.mean(diff)}\")\n",
    "\n",
    "\n",
    "def detailed_gradient_verification(computed_grads, load_data):\n",
    "    \"\"\"\n",
    "    Perform detailed verification of computed gradients\n",
    "    \"\"\"\n",
    "    # Check shapes first\n",
    "    print(\"Checking shapes...\")\n",
    "    print(f\"Computed grad_Fs_flat shape: {computed_grads['grad_Fs_flat'].shape}\")\n",
    "    print(f\"Expected grad_Fs_flat shape: {load_data['grad_Fs_flat'].shape}\")\n",
    "    \n",
    "    # Compute element-wise differences\n",
    "    diff = computed_grads['grad_Fs_flat'] - load_data['grad_Fs_flat']\n",
    "    abs_diff = np.abs(diff)\n",
    "    \n",
    "    print(\"\\nGradient difference statistics:\")\n",
    "    print(f\"Maximum absolute difference: {np.max(abs_diff):.10f}\")\n",
    "    print(f\"Average absolute difference: {np.mean(abs_diff):.10f}\")\n",
    "    print(f\"Median absolute difference: {np.median(abs_diff):.10f}\")\n",
    "    print(f\"Standard deviation of differences: {np.std(abs_diff):.10f}\")\n",
    "    \n",
    "    # Check for numerical instability\n",
    "    print(\"\\nChecking for potential numerical issues...\")\n",
    "    print(f\"Range of computed gradients: [{np.min(computed_grads['grad_Fs_flat']):.4f}, {np.max(computed_grads['grad_Fs_flat']):.4f}]\")\n",
    "    print(f\"Range of expected gradients: [{np.min(load_data['grad_Fs_flat']):.4f}, {np.max(load_data['grad_Fs_flat']):.4f}]\")\n",
    "    \n",
    "    # Find locations of largest differences\n",
    "    worst_indices = np.argsort(abs_diff.flatten())[-5:][::-1]\n",
    "    print(\"\\nTop 5 worst differences:\")\n",
    "    for idx in worst_indices:\n",
    "        i, j = np.unravel_index(idx, abs_diff.shape)\n",
    "        print(f\"Position ({i},{j}):\")\n",
    "        print(f\"  Computed value: {computed_grads['grad_Fs_flat'][i,j]:.10f}\")\n",
    "        print(f\"  Expected value: {load_data['grad_Fs_flat'][i,j]:.10f}\")\n",
    "        print(f\"  Absolute difference: {abs_diff[i,j]:.10f}\")\n",
    "\n",
    "def verify_implementation(data, forward_results, backward_results):\n",
    "    \"\"\"\n",
    "    Verify the implementation by checking intermediate values\n",
    "    \"\"\"\n",
    "    print(\"Verifying forward pass intermediates...\")\n",
    "    if 'conv_flat' in data:\n",
    "        conv_flat_diff = np.abs(forward_results['conv_flat'] - data['conv_flat'])\n",
    "        print(f\"conv_flat max difference: {np.max(conv_flat_diff):.10f}\")\n",
    "    \n",
    "    if 'P' in data:\n",
    "        P_diff = np.abs(forward_results['P'] - data['P'])\n",
    "        print(f\"P max difference: {np.max(P_diff):.10f}\")\n",
    "    \n",
    "    return detailed_gradient_verification(backward_results, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial test and gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_exercise2_data(data_path='../data/debug_info.npz'):\n",
    "    \"\"\"\n",
    "    Load the necessary data for Exercise 2\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the data file (default: 'Assignment3_data.npz')\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all necessary data and parameters\n",
    "    \"\"\"\n",
    "    # Load the data file\n",
    "    load_data = np.load(data_path, allow_pickle=True)\n",
    "    \n",
    "    # Extract required parameters and data\n",
    "    data = {\n",
    "        # Network parameters\n",
    "        'W1': load_data['W1'],          # shape: nh × (n_p * nf)\n",
    "        'W2': load_data['W2'],          # shape: 10 × nh\n",
    "        'b1': load_data['b1'],          # shape: nh × 1\n",
    "        'b2': load_data['b2'],          # shape: 10 × 1\n",
    "        \n",
    "        # Forward pass intermediates (for verification)\n",
    "        'conv_flat': load_data['conv_flat'],\n",
    "        'X1': load_data['X1'],\n",
    "        'P': load_data['P'],\n",
    "        \n",
    "        # Target labels\n",
    "        'Y': load_data['Y'],            # shape: 10 × n\n",
    "        \n",
    "        # Additional data needed for gradient computation\n",
    "        'conv_outputs_mat': load_data['conv_outputs_mat'],\n",
    "        'MX': load_data['MX'],\n",
    "        \n",
    "        # For gradient verification\n",
    "        'grad_Fs_flat': load_data['grad_Fs_flat']\n",
    "    }\n",
    "    \n",
    "    # Extract dimensions from the data\n",
    "    data['nf'] = load_data['nf'].item()    # number of filters\n",
    "    data['nh'] = load_data['nh'].item()    # number of hidden units\n",
    "    \n",
    "    # Manually calculate n_p (number of pixels)\n",
    "    # We can determine this from W1's shape: nh × (n_p * nf)\n",
    "    # First dimension of W1 should be nh, second should be (n_p * nf)\n",
    "    # So n_p = W1.shape[1] / nf\n",
    "    data['n_p'] = load_data['W1'].shape[1] // data['nf']\n",
    "    \n",
    "    # Calculate batch size n from Y's shape (10 × n)\n",
    "    data['n'] = load_data['Y'].shape[1]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = load_exercise2_data()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes:\n",
      "--------------------------------------------------\n",
      "W1              shape: (10, 128)\n",
      "W2              shape: (10, 10)\n",
      "b1              shape: (10, 1)\n",
      "b2              shape: (10, 1)\n",
      "conv_flat       shape: (128, 5)\n",
      "X1              shape: (1, 10, 5)\n",
      "P               shape: (10, 5)\n",
      "Y               shape: (10, 5)\n",
      "conv_outputs_mat shape: (64, 2, 5)\n",
      "MX              shape: (64, 48, 5)\n",
      "grad_Fs_flat    shape: (48, 2)\n",
      "nf              value: 2\n",
      "nh              value: 10\n",
      "n_p             value: 64\n",
      "n               value: 5\n"
     ]
    }
   ],
   "source": [
    "# Print shapes of all values in the data dictionary\n",
    "print(\"Data shapes:\")\n",
    "print(\"-\" * 50)\n",
    "for key, value in data.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"{key:15} shape: {value.shape}\")\n",
    "    elif isinstance(value, (int, float)):\n",
    "        print(f\"{key:15} value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 2.3614\n",
      "Verifying grad_Fs_flat...\n",
      "Maximum absolute difference: 2.220446049250313e-16\n",
      "Average absolute difference: 6.226934477308414e-17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run forward pass\n",
    "forward_results = forward_pass(\n",
    "    data['conv_outputs_mat'],\n",
    "    data['W1'],\n",
    "    data['W2'],\n",
    "    data['b1'],\n",
    "    data['b2'],\n",
    "    data['n_p'],\n",
    "    data['nf'],\n",
    "    data['n']\n",
    ")\n",
    "\n",
    "# Compute loss\n",
    "loss = compute_loss(data['Y'], forward_results['P'])\n",
    "print(f\"Cross-entropy loss: {loss:.4f}\")\n",
    "\n",
    "# Run backward pass and verify\n",
    "backward_results = backward_pass(\n",
    "    data['Y'],\n",
    "    forward_results['P'],\n",
    "    forward_results['h'],\n",
    "    forward_results['conv_flat'],\n",
    "    data['W1'],\n",
    "    data['W2'],\n",
    "    data['conv_outputs_mat'],\n",
    "    data['n_p'],\n",
    "    data['nf'],\n",
    "    data['n'],\n",
    "    data['MX']\n",
    ")\n",
    "\n",
    "# Verify the results\n",
    "verify_gradients(backward_results, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If gradients are not correct, run this to verify the implementation\n",
    "# verify_implementation(data, forward_results, backward_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skynet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
