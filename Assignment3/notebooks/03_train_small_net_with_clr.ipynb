{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Train small networks with cyclic learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# change working directory to the parent directory if not already there\n",
    "if os.path.basename(os.getcwd()) != 'Assignment3':\n",
    "    os.chdir(os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "\n",
    "from src.exercise3 import ConvolutionalNetwork, load_cifar_data, train_with_cyclical_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def LoadBatch(batch_id, dtype=np.float64):\n",
    "    \"\"\"\n",
    "    Load a CIFAR-10 batch file and return image data, one-hot labels, and raw labels.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): Path to the CIFAR-10 batch file.\n",
    "        dtype (type): Data type for image and one-hot encoded label arrays (float32 or float64).\n",
    "\n",
    "    Returns:\n",
    "        X (numpy.ndarray): Image data of shape (3072, 10000), type float32/float64, values in [0,1].\n",
    "        Y (numpy.ndarray): One-hot encoded labels of shape (10, 10000), type float32/float64.\n",
    "        y (numpy.ndarray): Label vector of shape (10000,), type int (values 0-9).\n",
    "    \"\"\"\n",
    "\n",
    "    cifar10_path = \"/Users/axhome/AX/KTH/Courses/DD2424-DeepLearning/Assignment3/data/cifar-10-batches-py\"\n",
    "    # Construct full file path\n",
    "    ## if batch_id is string\n",
    "    if isinstance(batch_id, str):\n",
    "        batch_file = os.path.join(cifar10_path, batch_id)\n",
    "    else:   \n",
    "        batch_file = os.path.join(cifar10_path, f\"data_batch_{batch_id}\")\n",
    "    \n",
    "    # Load the CIFAR-10 batch file\n",
    "    with open(batch_file, 'rb') as file:\n",
    "        batch = pickle.load(file, encoding='bytes')\n",
    "\n",
    "    # Extract image data and labels\n",
    "    images = batch[b'data']  # Shape (10000, 3072)\n",
    "    labels = np.array(batch[b'labels'])  # Shape (10000,)\n",
    "\n",
    "    # Convert image data to float and normalize to [0,1]\n",
    "    X = images.astype(dtype) / 255.0  # Shape (10000, 3072)\n",
    "\n",
    "    # Transpose X to match required shape (3072, 10000)\n",
    "    X = X.T  # Shape (3072, 10000)\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    K = 10  # Number of classes in CIFAR-10\n",
    "    Y = np.zeros((K, X.shape[1]), dtype=dtype)  # Shape (10, 10000)\n",
    "    Y[labels, np.arange(X.shape[1])] = 1  # Assign 1s for correct labels\n",
    "\n",
    "    # Return X (3072×10000), Y (10×10000), y (10000,)\n",
    "\n",
    "    # make sure labels are (1x10000)\n",
    "    labels = labels.reshape(1, len(labels))\n",
    "\n",
    "    return X, Y, labels\n",
    "\n",
    "\n",
    "def preprocessData(X_train_raw, X_val_raw, X_test_raw):\n",
    "    \"\"\"\n",
    "    Normalizes the dataset based on training set mean and standard deviation.\n",
    "    \n",
    "    Parameters:\n",
    "      X_train_raw, X_val_raw, X_test_raw: Raw data matrices\n",
    "    \n",
    "    Returns:\n",
    "      X_train, X_val, X_test: Normalized datasets\n",
    "    \"\"\"\n",
    "    X_train_mean = np.mean(X_train_raw, axis=1, keepdims=True)\n",
    "    X_train_std = np.std(X_train_raw, axis=1, keepdims=True)\n",
    "\n",
    "    X_train = (X_train_raw - X_train_mean) / X_train_std\n",
    "    X_val = (X_val_raw - X_train_mean) / X_train_std\n",
    "    X_test = (X_test_raw - X_train_mean) / X_train_std\n",
    "\n",
    "    return X_train, X_val, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cyclical_learning_rate(update_step, cycle_num, eta_min, eta_max, step_sizes):\n",
    "    \"\"\"\n",
    "    Computes the learning rate for the current update step using cyclical learning rates.\n",
    "    \n",
    "    Args:\n",
    "        update_step: Current step within the cycle\n",
    "        cycle_num: Current cycle number (0-indexed)\n",
    "        eta_min: Minimum learning rate\n",
    "        eta_max: Maximum learning rate\n",
    "        step_sizes: List of step sizes for each cycle\n",
    "    \n",
    "    Returns:\n",
    "        eta: Current learning rate\n",
    "    \"\"\"\n",
    "    # Get step size for current cycle\n",
    "    current_step_size = step_sizes[cycle_num]\n",
    "    \n",
    "    # Calculate local step within the cycle\n",
    "    cycle_step = update_step % (2 * current_step_size)\n",
    "    \n",
    "    # First half of the cycle: increase eta linearly\n",
    "    if cycle_step < current_step_size:\n",
    "        # Linear increase from eta_min to eta_max\n",
    "        return eta_min + (cycle_step / current_step_size) * (eta_max - eta_min)\n",
    "    # Second half of the cycle: decrease eta linearly \n",
    "    else:\n",
    "        # Linear decrease from eta_max to eta_min\n",
    "        return eta_max - ((cycle_step - current_step_size) / current_step_size) * (eta_max - eta_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_cyclical_lr(model, X_train, Y_train, X_val, Y_val, CLRparams, lambda_reg=0.0, \n",
    "                           use_label_smoothing=False, epsilon=0.1, logging_freq=10, verbose=True):\n",
    "    \"\"\"\n",
    "    Train the network using mini-batch gradient descent with cyclical learning rates.\n",
    "    \n",
    "    Args:\n",
    "        model: ConvolutionalNetwork instance\n",
    "        X_train: Training data, shape (input_dim, n_train)\n",
    "        Y_train: Training labels, shape (output_dim, n_train)\n",
    "        X_val: Validation data, shape (input_dim, n_val)\n",
    "        Y_val: Validation labels, shape (output_dim, n_val)\n",
    "        CLRparams: Dictionary containing:\n",
    "            - \"n_batch\": Mini-batch size\n",
    "            - \"eta_min\": Minimum learning rate\n",
    "            - \"eta_max\": Maximum learning rate\n",
    "            - \"n_s\": Initial step size (half cycle length)\n",
    "            - \"n_cycles\": Number of cycles to run\n",
    "        lambda_reg: L2 regularization parameter\n",
    "        use_label_smoothing: Whether to use label smoothing\n",
    "        epsilon: Label smoothing parameter\n",
    "        logging_freq: How often to log metrics\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        history: Dictionary containing training metrics\n",
    "    \"\"\"\n",
    "    # Extract parameters\n",
    "    batch_size = CLRparams[\"n_batch\"]\n",
    "    eta_min = CLRparams[\"eta_min\"]\n",
    "    eta_max = CLRparams[\"eta_max\"]\n",
    "    initial_step_size = CLRparams[\"n_s\"]\n",
    "    n_cycles = CLRparams[\"n_cycles\"]\n",
    "    \n",
    "    # Calculate step sizes for each cycle (doubling each time)\n",
    "    step_sizes = [initial_step_size * (2 ** i) for i in range(n_cycles)]\n",
    "    \n",
    "    # Apply label smoothing if requested\n",
    "    if use_label_smoothing:\n",
    "        Y_train_smooth = apply_label_smoothing(Y_train, epsilon)\n",
    "    else:\n",
    "        Y_train_smooth = Y_train\n",
    "    \n",
    "    # Get data dimensions\n",
    "    n_train = X_train.shape[1]\n",
    "    \n",
    "    # Calculate total updates\n",
    "    total_updates = sum(step_sizes) * 2\n",
    "    \n",
    "    # Initialize history dictionary\n",
    "    history = {\n",
    "        \"cost_train\": [], \"cost_val\": [],\n",
    "        \"loss_train\": [], \"loss_val\": [],\n",
    "        \"acc_train\": [], \"acc_val\": [],\n",
    "        \"update_steps\": [],\n",
    "        \"learning_rates\": []\n",
    "    }\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    update_step = 0\n",
    "    current_cycle = 0\n",
    "    cycle_step = 0\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Continue training until total updates reached\n",
    "    while update_step < total_updates:\n",
    "        # Shuffle data for each epoch\n",
    "        shuffle_idx = np.random.permutation(n_train)\n",
    "        X_shuffled = X_train[:, shuffle_idx]\n",
    "        Y_shuffled = Y_train_smooth[:, shuffle_idx]\n",
    "        \n",
    "        # Process mini-batches\n",
    "        for j in range(n_train // batch_size):\n",
    "            # Skip if we've done enough updates\n",
    "            if update_step >= total_updates:\n",
    "                break\n",
    "            \n",
    "            # Extract mini-batch\n",
    "            j_start = j * batch_size\n",
    "            j_end = min(j_start + batch_size, n_train)\n",
    "            X_batch = X_shuffled[:, j_start:j_end]\n",
    "            Y_batch = Y_shuffled[:, j_start:j_end]\n",
    "            \n",
    "            # Forward pass\n",
    "            P_batch, cache = model.forward(X_batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            grads = model.backward(Y_batch, P_batch, lambda_reg)\n",
    "            \n",
    "            # Get current learning rate\n",
    "            eta = get_cyclical_learning_rate(cycle_step, current_cycle, eta_min, eta_max, step_sizes)\n",
    "            \n",
    "            # Update model parameters\n",
    "            model.update_parameters(grads, eta)\n",
    "            \n",
    "            # Update cycle tracking\n",
    "            cycle_step += 1\n",
    "            update_step += 1\n",
    "            \n",
    "            # Check if cycle is complete\n",
    "            if cycle_step >= 2 * step_sizes[current_cycle]:\n",
    "                current_cycle += 1\n",
    "                cycle_step = 0\n",
    "                \n",
    "                # Break if all cycles complete\n",
    "                if current_cycle >= n_cycles:\n",
    "                    break\n",
    "            \n",
    "            # Log metrics periodically\n",
    "            if update_step % logging_freq == 0:\n",
    "                # Compute metrics\n",
    "                train_loss, train_acc = model.compute_loss_and_accuracy(X_train, Y_train, lambda_reg)\n",
    "                val_loss, val_acc = model.compute_loss_and_accuracy(X_val, Y_val, lambda_reg)\n",
    "                \n",
    "                # Store metrics\n",
    "                history[\"loss_train\"].append(train_loss)\n",
    "                history[\"acc_train\"].append(train_acc)\n",
    "                history[\"loss_val\"].append(val_loss)\n",
    "                history[\"acc_val\"].append(val_acc)\n",
    "                history[\"update_steps\"].append(update_step)\n",
    "                history[\"learning_rates\"].append(eta)\n",
    "                \n",
    "                # Print progress\n",
    "                if verbose and update_step % (logging_freq * 10) == 0:\n",
    "                    print(f\"Update {update_step}/{total_updates}, \"\n",
    "                          f\"Cycle {current_cycle+1}/{n_cycles}, \"\n",
    "                          f\"Step {cycle_step}/{2*step_sizes[current_cycle]}, \"\n",
    "                          f\"LR: {eta:.6f}, \"\n",
    "                          f\"Train Loss: {train_loss:.4f}, \"\n",
    "                          f\"Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Record training time\n",
    "    training_time = time.time() - start_time\n",
    "    history[\"training_time\"] = training_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "        print(f\"Final validation accuracy: {history['acc_val'][-1]:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Assignment3.src.utils import plot_architecture_comparison\n",
    "\n",
    "\n",
    "def compare_architectures(X_train, Y_train, X_val, Y_val, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    Compare the four architectures specified in Exercise 3.\n",
    "    \n",
    "    Args:\n",
    "        X_train, Y_train: Training data and labels\n",
    "        X_val, Y_val: Validation data and labels\n",
    "        X_test, Y_test: Test data and labels\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary with results for each architecture\n",
    "    \"\"\"\n",
    "    # Define the architectures to compare\n",
    "    architectures = [\n",
    "        {\"name\": \"Architecture 1\", \"f\": 2, \"nf\": 3, \"nh\": 50},\n",
    "        {\"name\": \"Architecture 2\", \"f\": 4, \"nf\": 10, \"nh\": 50},\n",
    "        {\"name\": \"Architecture 3\", \"f\": 8, \"nf\": 40, \"nh\": 50},\n",
    "        {\"name\": \"Architecture 4\", \"f\": 16, \"nf\": 160, \"nh\": 50}\n",
    "    ]\n",
    "    \n",
    "    # Common training parameters\n",
    "    clr_params = {\n",
    "        \"n_batch\": 100,\n",
    "        \"eta_min\": 1e-5,\n",
    "        \"eta_max\": 1e-1,\n",
    "        \"n_s\": 800,\n",
    "        \"n_cycles\": 3\n",
    "    }\n",
    "    \n",
    "    # Lambda value (from Exercise 2 or your previous experiments)\n",
    "    lambda_reg = 0.003\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Train and evaluate each architecture\n",
    "    for arch in architectures:\n",
    "        print(f\"\\nTraining {arch['name']}: f={arch['f']}, nf={arch['nf']}, nh={arch['nh']}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        model = ConvolutionalNetwork(\n",
    "            f=arch['f'], \n",
    "            nf=arch['nf'], \n",
    "            nh=arch['nh']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = train_with_cyclical_lr(\n",
    "            model, X_train, Y_train, X_val, Y_val,\n",
    "            clr_params, lambda_reg,\n",
    "            logging_freq=50, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_loss, test_acc = model.compute_loss_and_accuracy(X_test, Y_test, lambda_reg)\n",
    "        \n",
    "        # Store results\n",
    "        arch_result = {\n",
    "            \"name\": arch[\"name\"],\n",
    "            \"parameters\": arch,\n",
    "            \"history\": history,\n",
    "            \"test_accuracy\": test_acc,\n",
    "            \"training_time\": history[\"training_time\"]\n",
    "        }\n",
    "        results.append(arch_result)\n",
    "        \n",
    "        print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    plot_architecture_comparison(results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_longer_with_increasing_steps(X_train, Y_train, X_val, Y_val, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    Train architectures 2 and 3 for longer with increasing step sizes.\n",
    "    \n",
    "    Args:\n",
    "        X_train, Y_train: Training data and labels\n",
    "        X_val, Y_val: Validation data and labels\n",
    "        X_test, Y_test: Test data and labels\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary with training results\n",
    "    \"\"\"\n",
    "    # Define the architectures to train longer\n",
    "    architectures = [\n",
    "        {\"name\": \"Architecture 2\", \"f\": 4, \"nf\": 10, \"nh\": 50},\n",
    "        {\"name\": \"Architecture 3\", \"f\": 8, \"nf\": 40, \"nh\": 50},\n",
    "        {\"name\": \"Architecture 2 Wide\", \"f\": 4, \"nf\": 40, \"nh\": 50}\n",
    "    ]\n",
    "    \n",
    "    # Training parameters\n",
    "    clr_params = {\n",
    "        \"n_batch\": 100,\n",
    "        \"eta_min\": 1e-5,\n",
    "        \"eta_max\": 1e-1,\n",
    "        \"n_s\": 800,  # Initial step size\n",
    "        \"n_cycles\": 3\n",
    "    }\n",
    "    \n",
    "    lambda_reg = 0.003\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Train each architecture\n",
    "    for arch in architectures:\n",
    "        print(f\"\\nLonger training for {arch['name']}: f={arch['f']}, nf={arch['nf']}, nh={arch['nh']}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        model = ConvolutionalNetwork(\n",
    "            f=arch['f'], \n",
    "            nf=arch['nf'], \n",
    "            nh=arch['nh']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = train_with_cyclical_lr(\n",
    "            model, X_train, Y_train, X_val, Y_val,\n",
    "            clr_params, lambda_reg,\n",
    "            logging_freq=50, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_loss, test_acc = model.compute_loss_and_accuracy(X_test, Y_test, lambda_reg)\n",
    "        \n",
    "        # Store results\n",
    "        arch_result = {\n",
    "            \"name\": arch[\"name\"],\n",
    "            \"parameters\": arch,\n",
    "            \"history\": history,\n",
    "            \"test_accuracy\": test_acc,\n",
    "            \"training_time\": history[\"training_time\"]\n",
    "        }\n",
    "        results.append(arch_result)\n",
    "        \n",
    "        print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    plot_loss_curves_comparison(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_loss_curves_comparison(results):\n",
    "    \"\"\"\n",
    "    Plot loss and accuracy curves comparison for longer training with increasing step sizes.\n",
    "    \n",
    "    Args:\n",
    "        results: List of dictionaries with training results\n",
    "    \"\"\"\n",
    "    # Create figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot training loss\n",
    "    ax = axes[0, 0]\n",
    "    for res in results:\n",
    "        ax.plot(res[\"history\"][\"update_steps\"], res[\"history\"][\"loss_train\"], \n",
    "                label=f\"{res['name']} (Train)\")\n",
    "    ax.set_xlabel(\"Update Step\")\n",
    "    ax.set_ylabel(\"Training Loss\")\n",
    "    ax.set_title(\"Training Loss Comparison\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Plot validation loss\n",
    "    ax = axes[0, 1]\n",
    "    for res in results:\n",
    "        ax.plot(res[\"history\"][\"update_steps\"], res[\"history\"][\"loss_val\"], \n",
    "                label=f\"{res['name']} (Val)\")\n",
    "    ax.set_xlabel(\"Update Step\")\n",
    "    ax.set_ylabel(\"Validation Loss\")\n",
    "    ax.set_title(\"Validation Loss Comparison\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Plot training accuracy\n",
    "    ax = axes[1, 0]\n",
    "    for res in results:\n",
    "        ax.plot(res[\"history\"][\"update_steps\"], res[\"history\"][\"acc_train\"], \n",
    "                label=f\"{res['name']} (Train)\")\n",
    "    ax.set_xlabel(\"Update Step\")\n",
    "    ax.set_ylabel(\"Training Accuracy\")\n",
    "    ax.set_title(\"Training Accuracy Comparison\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Plot validation accuracy\n",
    "    ax = axes[1, 1]\n",
    "    for res in results:\n",
    "        ax.plot(res[\"history\"][\"update_steps\"], res[\"history\"][\"acc_val\"], \n",
    "                label=f\"{res['name']} (Val)\")\n",
    "    ax.set_xlabel(\"Update Step\")\n",
    "    ax.set_ylabel(\"Validation Accuracy\")\n",
    "    ax.set_title(\"Validation Accuracy Comparison\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('longer_training_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Also create a bar chart for final test accuracies\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    names = [res[\"name\"] for res in results]\n",
    "    test_accs = [res[\"test_accuracy\"] for res in results]\n",
    "    \n",
    "    bars = plt.bar(names, test_accs, color='skyblue')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.title('Test Accuracy Comparison - Longer Training')\n",
    "    plt.ylim([0, 1])\n",
    "    \n",
    "    # Add accuracy values on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.4f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('longer_training_test_accuracy.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_architecture_comparison(results):\n",
    "    \"\"\"\n",
    "    Plot comparison of different architectures.\n",
    "    \n",
    "    Args:\n",
    "        results: List of dictionaries with architecture results\n",
    "    \"\"\"\n",
    "    # Extract data for plotting\n",
    "    names = [res[\"name\"] for res in results]\n",
    "    test_accs = [res[\"test_accuracy\"] for res in results]\n",
    "    train_times = [res[\"training_time\"] for res in results]\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot test accuracy\n",
    "    bar_width = 0.7\n",
    "    x = np.arange(len(names))\n",
    "    bars1 = ax1.bar(x, test_accs, bar_width, color='skyblue')\n",
    "    \n",
    "    ax1.set_ylabel('Test Accuracy')\n",
    "    ax1.set_title('Test Accuracy by Architecture')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(names, rotation=45, ha='right')\n",
    "    ax1.set_ylim([0, 1])\n",
    "    \n",
    "    # Add accuracy values on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.4f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot training time\n",
    "    bars2 = ax2.bar(x, train_times, bar_width, color='salmon')\n",
    "    \n",
    "    ax2.set_ylabel('Training Time (seconds)')\n",
    "    ax2.set_title('Training Time by Architecture')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(names, rotation=45, ha='right')\n",
    "    \n",
    "    # Add time values on bars\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{height:.1f}s', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('architecture_comparison.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exercise_3():\n",
    "    \"\"\"\n",
    "    Run Exercise 3 of Assignment 3\n",
    "    \"\"\"\n",
    "    print(\"Running Exercise 3 - Training Small Networks with Cyclical Learning Rates\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    X_train_raw, Y_train, y_train = LoadBatch(1)\n",
    "    X_val_raw, Y_val, y_val = LoadBatch(2)  \n",
    "    X_test_raw, Y_test, y_test = LoadBatch(\"test_batch\")\n",
    "    \n",
    "    # Normalize data\n",
    "    X_train, X_val, X_test = preprocessData(X_train_raw, X_val_raw, X_test_raw)\n",
    "    \n",
    "    # Compare architectures\n",
    "    print(\"\\nPart 1: Comparing different architectures...\")\n",
    "    arch_results = compare_architectures(X_train, Y_train, X_val, Y_val, X_test, Y_test)\n",
    "    \n",
    "    # Train longer with increasing step sizes\n",
    "    print(\"\\nPart 2: Training with longer cycles and increasing step sizes...\")\n",
    "    longer_results = train_longer_with_increasing_steps(X_train, Y_train, X_val, Y_val, X_test, Y_test)\n",
    "    \n",
    "    print(\"\\nExercise 3 completed!\")\n",
    "    \n",
    "    return {\n",
    "        \"architecture_comparison\": arch_results,\n",
    "        \"longer_training\": longer_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Exercise 3 - Training Small Networks with Cyclical Learning Rates\n",
      "Loading and preprocessing data...\n",
      "\n",
      "Part 1: Comparing different architectures...\n",
      "\n",
      "Training Architecture 1: f=2, nf=3, nh=50\n",
      "Update 500/11200, Cycle 1/3, Step 500/1600, LR: 0.062379, Train Loss: 2.3022, Val Acc: 0.1010\n",
      "Update 1000/11200, Cycle 1/3, Step 1000/1600, LR: 0.075127, Train Loss: 2.3022, Val Acc: 0.1008\n",
      "Update 1500/11200, Cycle 1/3, Step 1500/1600, LR: 0.012634, Train Loss: 2.3022, Val Acc: 0.1010\n",
      "Update 2000/11200, Cycle 2/3, Step 400/3200, LR: 0.024945, Train Loss: 2.3022, Val Acc: 0.1008\n",
      "Update 2500/11200, Cycle 2/3, Step 900/3200, LR: 0.056192, Train Loss: 2.3022, Val Acc: 0.1008\n",
      "Update 3000/11200, Cycle 2/3, Step 1400/3200, LR: 0.087439, Train Loss: 2.3022, Val Acc: 0.1010\n",
      "Update 3500/11200, Cycle 2/3, Step 1900/3200, LR: 0.081314, Train Loss: 2.3022, Val Acc: 0.1010\n",
      "Update 4000/11200, Cycle 2/3, Step 2400/3200, LR: 0.050067, Train Loss: 2.3022, Val Acc: 0.1008\n",
      "Update 4500/11200, Cycle 2/3, Step 2900/3200, LR: 0.018821, Train Loss: 2.3022, Val Acc: 0.1010\n",
      "Update 5000/11200, Cycle 3/3, Step 200/6400, LR: 0.006228, Train Loss: 2.3022, Val Acc: 0.1010\n",
      "Update 5500/11200, Cycle 3/3, Step 700/6400, LR: 0.021852, Train Loss: 2.3022, Val Acc: 0.1010\n",
      "Update 6000/11200, Cycle 3/3, Step 1200/6400, LR: 0.037475, Train Loss: 2.3022, Val Acc: 0.1010\n",
      "Update 6500/11200, Cycle 3/3, Step 1700/6400, LR: 0.053098, Train Loss: 2.3022, Val Acc: 0.1010\n",
      "Update 7000/11200, Cycle 3/3, Step 2200/6400, LR: 0.068722, Train Loss: 2.3022, Val Acc: 0.1010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run Exercise 3\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mrun_exercise_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Print summary of results\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSummary of Results:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mrun_exercise_3\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Compare architectures\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPart 1: Comparing different architectures...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m arch_results = \u001b[43mcompare_architectures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Train longer with increasing step sizes\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPart 2: Training with longer cycles and increasing step sizes...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mcompare_architectures\u001b[39m\u001b[34m(X_train, Y_train, X_val, Y_val, X_test, Y_test)\u001b[39m\n\u001b[32m     41\u001b[39m model = ConvolutionalNetwork(\n\u001b[32m     42\u001b[39m     f=arch[\u001b[33m'\u001b[39m\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m     43\u001b[39m     nf=arch[\u001b[33m'\u001b[39m\u001b[33mnf\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m     44\u001b[39m     nh=arch[\u001b[33m'\u001b[39m\u001b[33mnh\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     45\u001b[39m )\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m history = \u001b[43mtrain_with_cyclical_lr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclr_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     52\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[32m     55\u001b[39m test_loss, test_acc = model.compute_loss_and_accuracy(X_test, Y_test, lambda_reg)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 114\u001b[39m, in \u001b[36mtrain_with_cyclical_lr\u001b[39m\u001b[34m(model, X_train, Y_train, X_val, Y_val, CLRparams, lambda_reg, use_label_smoothing, epsilon, logging_freq, verbose)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m update_step % logging_freq == \u001b[32m0\u001b[39m:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# Compute metrics\u001b[39;00m\n\u001b[32m    113\u001b[39m     train_loss, train_acc = model.compute_loss_and_accuracy(X_train, Y_train, lambda_reg)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     val_loss, val_acc = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss_and_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# Store metrics\u001b[39;00m\n\u001b[32m    117\u001b[39m     history[\u001b[33m\"\u001b[39m\u001b[33mloss_train\u001b[39m\u001b[33m\"\u001b[39m].append(train_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AX/KTH/Courses/DD2424-DeepLearning/Assignment3/src/network.py:145\u001b[39m, in \u001b[36mConvolutionalNetwork.compute_loss_and_accuracy\u001b[39m\u001b[34m(self, X, Y, lambda_reg)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03mCompute loss and accuracy for the given data with added numerical stability.\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    142\u001b[39m \u001b[33;03m    accuracy: Classification accuracy\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m P, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# Prevent numerical instability\u001b[39;00m\n\u001b[32m    148\u001b[39m epsilon = \u001b[32m1e-15\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AX/KTH/Courses/DD2424-DeepLearning/Assignment3/src/network.py:65\u001b[39m, in \u001b[36mConvolutionalNetwork.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     62\u001b[39m n = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Forward convolution\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m conv_outputs, MX = \u001b[43mforward_convolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mFs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Forward pass through fully connected layers\u001b[39;00m\n\u001b[32m     68\u001b[39m forward_results = forward_pass(\n\u001b[32m     69\u001b[39m     conv_outputs, \n\u001b[32m     70\u001b[39m     \u001b[38;5;28mself\u001b[39m.W1, \n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m     n\n\u001b[32m     77\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AX/KTH/Courses/DD2424-DeepLearning/Assignment3/src/convolution.py:186\u001b[39m, in \u001b[36mforward_convolution\u001b[39m\u001b[34m(X, Fs)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[33;03mPerform the forward convolution using the most efficient method (einsum).\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m \u001b[33;03m    MX: Matrix of patches, shape (n_p, f*f*3, n) - needed for backward pass\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    185\u001b[39m X_ims = reshape_input(X)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m MX = \u001b[43mcreate_patch_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_ims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# Use the most efficient method (einsum)\u001b[39;00m\n\u001b[32m    189\u001b[39m conv_outputs = convolution_einsum(MX, Fs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AX/KTH/Courses/DD2424-DeepLearning/Assignment3/src/convolution.py:82\u001b[39m, in \u001b[36mcreate_patch_matrix\u001b[39m\u001b[34m(X_ims, f)\u001b[39m\n\u001b[32m     79\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[32m32\u001b[39m, f):\n\u001b[32m     80\u001b[39m             \u001b[38;5;66;03m# Extract patch and reshape it to a row vector\u001b[39;00m\n\u001b[32m     81\u001b[39m             patch = X_ims[h:h+f, w:w+f, :, i]\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m             MX[patch_idx, :, i] = \u001b[43mpatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m             patch_idx += \u001b[32m1\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m MX\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run Exercise 3\n",
    "results = run_exercise_3()\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nSummary of Results:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nArchitecture Comparison:\")\n",
    "for arch, metrics in results[\"architecture_comparison\"].items():\n",
    "    print(f\"\\n{arch}:\")\n",
    "    print(f\"Final Validation Accuracy: {metrics['val_acc']:.4f}\")\n",
    "    print(f\"Training Time: {metrics['time']:.2f} seconds\")\n",
    "\n",
    "print(\"\\nLonger Training Results:\")\n",
    "print(f\"Final Validation Accuracy: {results['longer_training']['val_acc']:.4f}\")\n",
    "print(f\"Total Training Time: {results['longer_training']['time']:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skynet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
